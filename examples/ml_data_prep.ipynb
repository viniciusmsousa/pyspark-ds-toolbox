{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark_ds_toolbox.ml import data_prep as ml_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/18 16:23:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/18 16:23:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/18 16:23:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/01/18 16:23:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/01/18 16:23:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Spark-Toolbox') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_classifier_weights()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "| id_conta|target_value|        p1|\n",
      "+---------+------------+----------+\n",
      "|484034448|           0|0.54177165|\n",
      "|418564110|           0| 0.7748305|\n",
      "|464339157|           0|0.22917716|\n",
      "|309485972|           0|0.60101485|\n",
      "|154315670|           0|0.48498958|\n",
      "+---------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pd.read_csv('../tests/data/df_test_binary_classifier_decile_analysis.csv'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------------------+\n",
      "| id_conta|target_value|        p1|weight_target_value|\n",
      "+---------+------------+----------+-------------------+\n",
      "|484034448|           0|0.54177165| 0.5151898734177215|\n",
      "|418564110|           0| 0.7748305| 0.5151898734177215|\n",
      "|464339157|           0|0.22917716| 0.5151898734177215|\n",
      "|309485972|           0|0.60101485| 0.5151898734177215|\n",
      "|154315670|           0|0.48498958| 0.5151898734177215|\n",
      "+---------+------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_weights = ml_dp.class_weights.binary_classifier_weights(dfs=df, col_target='target_value')\n",
    "dfs_weights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_features_vector()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+----+\n",
      "|index|num1|num2|cat1|cat2|\n",
      "+-----+----+----+----+----+\n",
      "|    1| 0.1| 0.4|   a|   c|\n",
      "|    2| 0.2| 0.3|   b|   d|\n",
      "|    3| 0.3| 0.2|   a|   c|\n",
      "|    4| 0.4| 0.1|   b|   d|\n",
      "+-----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'index':[1, 2, 3, 4],\n",
    "    'num1': [0.1, 0.2, 0.3, 0.4],\n",
    "    'num2': [0.4, 0.3, 0.2, 0.1],\n",
    "    'cat1': ['a', 'b', 'a', 'b'],\n",
    "    'cat2': ['c', 'd', 'c', 'd']\n",
    "    })\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+----+-----------------+\n",
      "|index|num1|num2|cat1|cat2|         features|\n",
      "+-----+----+----+----+----+-----------------+\n",
      "|    1| 0.1| 0.4|   a|   c|[0.1,0.4,0.0,0.0]|\n",
      "|    2| 0.2| 0.3|   b|   d|[0.2,0.3,1.0,1.0]|\n",
      "|    3| 0.3| 0.2|   a|   c|[0.3,0.2,0.0,0.0]|\n",
      "|    4| 0.4| 0.1|   b|   d|[0.4,0.1,1.0,1.0]|\n",
      "+-----+----+----+----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = ml_dp.features_vector.get_features_vector(\n",
    "    df=df,\n",
    "    num_features=['num1', 'num2'],\n",
    "    cat_features=['cat1', 'cat2']\n",
    ")\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
