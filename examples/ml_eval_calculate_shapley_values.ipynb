{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vini/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType, StructField, StructType, StringType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "import pyspark.ml.feature as FF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_ds_toolbox.ml.eval import get_p1\n",
    "\n",
    "from pyspark_ds_toolbox.ml.eval import calculate_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/04 08:42:02 WARN Utils: Your hostname, matrix.local resolves to a loopback address: 127.0.0.1; using 10.0.0.105 instead (on interface en0)\n",
      "21/12/04 08:42:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/vini/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/12/04 08:42:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Ml-Pipes') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file): \n",
    "    return pd.read_stata(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n",
    "\n",
    "df = read_data('nsw_mixtape.dta')\n",
    "df = pd.concat((df, read_data('cps_mixtape.dta')))\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = spark.createDataFrame(df.drop(columns=['data_id']))\\\n",
    "    .withColumn('age2', F.col('age')**2)\\\n",
    "    .withColumn('age3', F.col('age')**3)\\\n",
    "    .withColumn('educ2', F.col('educ')**2)\\\n",
    "    .withColumn('educ_re74', F.col('educ')*F.col('re74'))\\\n",
    "    .withColumn('u74', F.when(F.col('re74')==0, 1).otherwise(0))\\\n",
    "    .withColumn('u75', F.when(F.col('re75')==0, 1).otherwise(0))\n",
    "\n",
    "features=['age', 'age2', 'age3', 'educ', 'educ2', 'marr', 'nodegree', 'black', 'hisp', 're74', 're75', 'u74', 'u75', 'educ_re74']\n",
    "assembler = FF.VectorAssembler(inputCols=features, outputCol='features')\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "df_assembled = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- treat: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- educ: double (nullable = true)\n",
      " |-- black: double (nullable = true)\n",
      " |-- hisp: double (nullable = true)\n",
      " |-- marr: double (nullable = true)\n",
      " |-- nodegree: double (nullable = true)\n",
      " |-- re74: double (nullable = true)\n",
      " |-- re75: double (nullable = true)\n",
      " |-- re78: double (nullable = true)\n",
      " |-- age2: double (nullable = true)\n",
      " |-- age3: double (nullable = true)\n",
      " |-- educ2: double (nullable = true)\n",
      " |-- educ_re74: double (nullable = true)\n",
      " |-- u74: integer (nullable = false)\n",
      " |-- u75: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: float (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size=0.8\n",
    "train, test = df_assembled.randomSplit([train_size, (1-train_size)], seed=12345)\n",
    "\n",
    "model = GBTClassifier(labelCol='treat')\n",
    "p = Pipeline(stages=[model])\n",
    "p_fitted = p.fit(train)\n",
    "\n",
    "df_predicted = p_fitted.transform(test).withColumn('probability', get_p1(F.col('probability')))\n",
    "\n",
    "df_predicted.printSchema()\n",
    "v = df_predicted.filter(F.col('index')==3).select('probability').collect()[0][0]\n",
    "m = df_predicted.select('probability').toPandas().probability.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from psutil import virtual_memory\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F, SparkSession, types as T, Window\n",
    "\n",
    "import operator\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shapley_values(\n",
    "        spark,\n",
    "        df,\n",
    "        model,\n",
    "        row_of_interest,\n",
    "        feature_names,\n",
    "        features_col='features',\n",
    "        column_to_examine='anomalyScore',\n",
    "        print_shap_values=False\n",
    "):\n",
    "    \"\"\"\n",
    "    # Based on the algorithm described here:\n",
    "    # https://christophm.github.io/interpretable-ml-book/shapley.html#estimating-the-shapley-value\n",
    "    # And on Baskerville's implementation for IForest/ AnomalyModel here:\n",
    "    # https://github.com/equalitie/baskerville/blob/develop/src/baskerville/util/model_interpretation/helpers.py#L235\n",
    "    \"\"\"\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField('id', IntegerType(), True),\n",
    "        StructField('feature', StringType(), True),\n",
    "        StructField('shap', FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    results = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n",
    "\n",
    "    features_perm_col = 'features_permutations'\n",
    "    marginal_contribution_filter = F.avg('marginal_contribution').alias('shap_value')\n",
    "    # broadcast the row of interest and ordered feature names\n",
    "    ROW_OF_INTEREST_BROADCAST = spark.sparkContext.broadcast(row_of_interest)\n",
    "    ORDERED_FEATURE_NAMES = spark.sparkContext.broadcast(feature_names)\n",
    "\n",
    "    # persist before continuing with calculations\n",
    "    if not df.is_cached:\n",
    "        df = df.persist()\n",
    "\n",
    "    # get permutations\n",
    "    # Creates a column for the ordered features and then shuffles it.\n",
    "    # The result is a dataframe with a column `output_col` that contains:\n",
    "    # [feat2, feat4, feat3, feat1],\n",
    "    # [feat3, feat4, feat2, feat1],\n",
    "    # [feat1, feat2, feat4, feat3],\n",
    "    # ...\n",
    "    features_df = df.withColumn(\n",
    "        'features_permutations',\n",
    "        F.shuffle(\n",
    "            F.array(*[F.lit(f) for f in feature_names])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # set up the udf - x-j and x+j need to be calculated for every row\n",
    "    def calculate_x(\n",
    "            feature_j, z_features, curr_feature_perm\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The instance  x+j is the instance of interest,\n",
    "        but all values in the order before feature j are\n",
    "        replaced by feature values from the sample z\n",
    "        The instance  xâˆ’j is the same as  x+j, but in addition\n",
    "        has feature j replaced by the value for feature j from the sample z\n",
    "        \"\"\"\n",
    "        x_interest = ROW_OF_INTEREST_BROADCAST.value\n",
    "        ordered_features = ORDERED_FEATURE_NAMES.value\n",
    "        x_minus_j = list(z_features).copy()\n",
    "        x_plus_j = list(z_features).copy()\n",
    "        f_i = curr_feature_perm.index(feature_j)\n",
    "        after_j = False\n",
    "        for f in curr_feature_perm[f_i:]:\n",
    "            # replace z feature values with x of interest feature values\n",
    "            # iterate features in current permutation until one before j\n",
    "            # x-j = [z1, z2, ... zj-1, xj, xj+1, ..., xN]\n",
    "            # we already have zs because we go row by row with the udf,\n",
    "            # so replace z_features with x of interest\n",
    "            f_index = ordered_features.index(f)\n",
    "            new_value = x_interest[f_index]\n",
    "            x_plus_j[f_index] = new_value\n",
    "            if after_j:\n",
    "                x_minus_j[f_index] = new_value\n",
    "            after_j = True\n",
    "\n",
    "        # minus must be first because of lag\n",
    "        return Vectors.dense(x_minus_j), Vectors.dense(x_plus_j)\n",
    "\n",
    "    udf_calculate_x = F.udf(calculate_x, T.ArrayType(VectorUDT()))\n",
    "\n",
    "    # persist before processing\n",
    "    features_df = features_df.persist()\n",
    "\n",
    "    for f in feature_names:\n",
    "        # x column contains x-j and x+j in this order.\n",
    "        # Because lag is calculated this way:\n",
    "        # F.col('anomalyScore') - (F.col('anomalyScore') one row before)\n",
    "        # x-j needs to be first in `x` column array so we should have:\n",
    "        # id1, [x-j row i,  x+j row i]\n",
    "        # ...\n",
    "        # that with explode becomes:\n",
    "        # id1, x-j row i\n",
    "        # id1, x+j row i\n",
    "        # ...\n",
    "        # to give us (x+j - x-j) when we calculate marginal contribution\n",
    "        # Note that with explode, x-j and x+j for the same row have the same id\n",
    "        # This gives us the opportunity to use lag with\n",
    "        # a window partitioned by id\n",
    "        x_df = features_df.withColumn('x', udf_calculate_x(\n",
    "            F.lit(f), features_col, features_perm_col\n",
    "        )).persist()\n",
    "\n",
    "        # Calculating SHAP values for f\n",
    "        x_df = x_df.selectExpr(\n",
    "            'id', f'explode(x) as {features_col}'\n",
    "        ).cache()\n",
    "        x_df = model.transform(x_df).withColumn('probability', get_p1(F.col('probability')))\n",
    "\n",
    "        # marginal contribution is calculated using a window and a lag of 1.\n",
    "        # the window is partitioned by id because x+j and x-j for the same row\n",
    "        # will have the same id\n",
    "        x_df = x_df.withColumn(\n",
    "            'marginal_contribution',\n",
    "            F.col(column_to_examine) - F.lag(F.col(column_to_examine), 1).over(Window.partitionBy('id').orderBy('id'))\n",
    "        )\n",
    "        # calculate the average\n",
    "        x_df = x_df.filter(x_df.marginal_contribution.isNotNull())\n",
    "        \n",
    "        feat_shap_value = pd.DataFrame.from_dict({\n",
    "            'id': [row_of_interest['id']],\n",
    "            'feature': [f],\n",
    "            'shap_value': [x_df.select(marginal_contribution_filter).first().shap_value]\n",
    "        })\n",
    "        feat_shap_value = spark.createDataFrame(feat_shap_value)\n",
    "        if print_shap_values:\n",
    "            print(f'Marginal Contribution for feature: {f} = {x_df.select(marginal_contribution_filter).first().shap_value}')\n",
    "        \n",
    "        results = results.union(feat_shap_value)\n",
    "        break\n",
    "        \n",
    "    return (results, x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: age = -0.003291975620736412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, r = calculate_shapley_values(\n",
    "    spark=spark,\n",
    "    df = df_predicted,\n",
    "    id_column='index',\n",
    "    model = p_fitted,\n",
    "    row_of_interest = df_predicted.filter(F.col('index')==3).first(),\n",
    "    feature_names = features,\n",
    "    features_col='features',\n",
    "    column_to_examine='probability',\n",
    "    print_shap_values=True\n",
    ")\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+----------+---------------------+\n",
      "|index|            features|       rawPrediction|probability|prediction|marginal_contribution|\n",
      "+-----+--------------------+--------------------+-----------+----------+---------------------+\n",
      "| 1697|[3.0,1156.0,48.0,...|[1.53137828714336...| 0.04466992|       0.0|         -0.012413949|\n",
      "| 2250|[3.0,0.0,91125.0,...|[1.72242596217247...|0.030922757|       0.0|        -0.0135901775|\n",
      "| 2509|[3.0,400.0,8000.0...|[1.55068795525105...|0.043050535|       0.0|        -2.1320954E-4|\n",
      "| 2927|[3.0,0.0,48.0,6.0...|[1.82098708167501...|0.025531625|       0.0|         -0.004834337|\n",
      "| 5556|[3.0,1156.0,39304...|[1.55689123243229...| 0.04254231|       0.0|         -0.002766002|\n",
      "| 8484|[3.0,1521.0,48.0,...|[1.66132311714578...|0.034802407|       0.0|         -0.009710528|\n",
      "|10959|[3.0,361.0,6859.0...|[1.55626474703821...|0.042593375|       0.0|        -0.0052779615|\n",
      "|11625|[3.0,841.0,24389....|[1.14831830698900...|  0.0914019|       0.0|           0.04694729|\n",
      "|12568|[3.0,0.0,48.0,6.0...|[1.62373532676677...| 0.03741788|       0.0|         -0.011991646|\n",
      "|15173|[3.0,0.0,48.0,6.0...|[1.51276071103555...| 0.04628613|       0.0|          0.024334084|\n",
      "|15237|[3.0,0.0,48.0,6.0...|[1.59815480999448...|0.039304834|       0.0|         -0.007910669|\n",
      "|15371|[3.0,0.0,48.0,12....|[0.50693030839372...|   0.266225|       0.0|           0.17152233|\n",
      "| 3009|[3.0,841.0,24389....|[1.46243737231760...|0.050937526|       0.0|         0.0030804276|\n",
      "| 5934|[3.0,0.0,5832.0,1...|[1.54384732035766...|0.043617703|       0.0|                  0.0|\n",
      "| 7436|[3.0,0.0,9261.0,6...|[0.64056538232603...|  0.2173578|       0.0|          0.038481995|\n",
      "| 9179|[3.0,729.0,48.0,6...|[0.81006776771024...| 0.16518618|       0.0|         -0.054095805|\n",
      "|10237|[3.0,0.0,48.0,6.0...|[1.59815480999448...|0.039304834|       0.0|         -0.007910669|\n",
      "|11309|[3.0,0.0,157464.0...|[1.51428049261055...|0.046152133|       0.0|         0.0020507015|\n",
      "|11860|[3.0,0.0,48.0,11....|[1.42316067755153...|0.054871783|       0.0|          0.013381347|\n",
      "|13299|[3.0,784.0,48.0,6...|[1.55626474703821...|0.042593375|       0.0|        -0.0029232763|\n",
      "+-----+--------------------+--------------------+-----------+----------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`id`' given input columns: [features, index, marginal_contribution, prediction, probability, rawPrediction];\n'Project ['id]\n+- Filter isnotnull(marginal_contribution#1768)\n   +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747, marginal_contribution#1768]\n      +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747, _we0#1769, (probability#1762 - _we0#1769) AS marginal_contribution#1768]\n         +- Window [lag(probability#1762, -1, null) windowspecdefinition(index#0L, index#0L ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we0#1769], [index#0L], [index#0L ASC NULLS FIRST]\n            +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747]\n               +- Project [index#0L, features#1369, rawPrediction#1729, <lambda>(probability#1736) AS probability#1762, prediction#1747]\n                  +- Project [index#0L, features#1369, rawPrediction#1729, probability#1736, UDF(rawPrediction#1729) AS prediction#1747]\n                     +- Project [index#0L, features#1369, rawPrediction#1729, UDF(rawPrediction#1729) AS probability#1736]\n                        +- Project [index#0L, features#1369, UDF(features#1369) AS rawPrediction#1729]\n                           +- Project [index#0L, features#1369]\n                              +- Generate explode(x#898), false, [features#1369]\n                                 +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#263, prediction#216, features_permutations#449, calculate_x(age, features#118, features_permutations#449) AS x#898]\n                                    +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#263, prediction#216, shuffle(array(age, age2, age3, educ, educ2, marr, nodegree, black, hisp, re74, re75, u74, u75, educ_re74), Some(-2092057485210354459)) AS features_permutations#449]\n                                       +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, <lambda>(probability#189) AS probability#263, prediction#216]\n                                          +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#189, UDF(rawPrediction#166) AS prediction#216]\n                                             +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, UDF(rawPrediction#166) AS probability#189]\n                                                +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, UDF(features#118) AS rawPrediction#166]\n                                                   +- Sample 0.8, 1.0, false, 12345\n                                                      +- Sort [index#0L ASC NULLS FIRST, treat#1 ASC NULLS FIRST, age#2 ASC NULLS FIRST, educ#3 ASC NULLS FIRST, black#4 ASC NULLS FIRST, hisp#5 ASC NULLS FIRST, marr#6 ASC NULLS FIRST, nodegree#7 ASC NULLS FIRST, re74#8 ASC NULLS FIRST, re75#9 ASC NULLS FIRST, re78#10 ASC NULLS FIRST, age2#22 ASC NULLS FIRST, age3#35 ASC NULLS FIRST, educ2#49 ASC NULLS FIRST, educ_re74#64 ASC NULLS FIRST, u74#80 ASC NULLS FIRST, u75#97 ASC NULLS FIRST, features#118 ASC NULLS FIRST], false\n                                                         +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, UDF(struct(age, age#2, age2, age2#22, age3, age3#35, educ, educ#3, educ2, educ2#49, marr, marr#6, nodegree, nodegree#7, black, black#4, hisp, hisp#5, re74, re74#8, re75, re75#9, u74_double_VectorAssembler_e8cf33225a47, cast(u74#80 as double), ... 4 more fields)) AS features#118]\n                                                            +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, CASE WHEN (re75#9 = cast(0 as double)) THEN 1 ELSE 0 END AS u75#97]\n                                                               +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, CASE WHEN (re74#8 = cast(0 as double)) THEN 1 ELSE 0 END AS u74#80]\n                                                                  +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, (educ#3 * re74#8) AS educ_re74#64]\n                                                                     +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, POWER(educ#3, cast(2 as double)) AS educ2#49]\n                                                                        +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, POWER(age#2, cast(3 as double)) AS age3#35]\n                                                                           +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, POWER(age#2, cast(2 as double)) AS age2#22]\n                                                                              +- LogicalRDD [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f5/frm4xmyd25bdq89lxc8bq31r0000gn/T/ipykernel_5698/4151734664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`id`' given input columns: [features, index, marginal_contribution, prediction, probability, rawPrediction];\n'Project ['id]\n+- Filter isnotnull(marginal_contribution#1768)\n   +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747, marginal_contribution#1768]\n      +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747, _we0#1769, (probability#1762 - _we0#1769) AS marginal_contribution#1768]\n         +- Window [lag(probability#1762, -1, null) windowspecdefinition(index#0L, index#0L ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we0#1769], [index#0L], [index#0L ASC NULLS FIRST]\n            +- Project [index#0L, features#1369, rawPrediction#1729, probability#1762, prediction#1747]\n               +- Project [index#0L, features#1369, rawPrediction#1729, <lambda>(probability#1736) AS probability#1762, prediction#1747]\n                  +- Project [index#0L, features#1369, rawPrediction#1729, probability#1736, UDF(rawPrediction#1729) AS prediction#1747]\n                     +- Project [index#0L, features#1369, rawPrediction#1729, UDF(rawPrediction#1729) AS probability#1736]\n                        +- Project [index#0L, features#1369, UDF(features#1369) AS rawPrediction#1729]\n                           +- Project [index#0L, features#1369]\n                              +- Generate explode(x#898), false, [features#1369]\n                                 +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#263, prediction#216, features_permutations#449, calculate_x(age, features#118, features_permutations#449) AS x#898]\n                                    +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#263, prediction#216, shuffle(array(age, age2, age3, educ, educ2, marr, nodegree, black, hisp, re74, re75, u74, u75, educ_re74), Some(-2092057485210354459)) AS features_permutations#449]\n                                       +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, <lambda>(probability#189) AS probability#263, prediction#216]\n                                          +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, probability#189, UDF(rawPrediction#166) AS prediction#216]\n                                             +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, rawPrediction#166, UDF(rawPrediction#166) AS probability#189]\n                                                +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, features#118, UDF(features#118) AS rawPrediction#166]\n                                                   +- Sample 0.8, 1.0, false, 12345\n                                                      +- Sort [index#0L ASC NULLS FIRST, treat#1 ASC NULLS FIRST, age#2 ASC NULLS FIRST, educ#3 ASC NULLS FIRST, black#4 ASC NULLS FIRST, hisp#5 ASC NULLS FIRST, marr#6 ASC NULLS FIRST, nodegree#7 ASC NULLS FIRST, re74#8 ASC NULLS FIRST, re75#9 ASC NULLS FIRST, re78#10 ASC NULLS FIRST, age2#22 ASC NULLS FIRST, age3#35 ASC NULLS FIRST, educ2#49 ASC NULLS FIRST, educ_re74#64 ASC NULLS FIRST, u74#80 ASC NULLS FIRST, u75#97 ASC NULLS FIRST, features#118 ASC NULLS FIRST], false\n                                                         +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, u75#97, UDF(struct(age, age#2, age2, age2#22, age3, age3#35, educ, educ#3, educ2, educ2#49, marr, marr#6, nodegree, nodegree#7, black, black#4, hisp, hisp#5, re74, re74#8, re75, re75#9, u74_double_VectorAssembler_e8cf33225a47, cast(u74#80 as double), ... 4 more fields)) AS features#118]\n                                                            +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, u74#80, CASE WHEN (re75#9 = cast(0 as double)) THEN 1 ELSE 0 END AS u75#97]\n                                                               +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, educ_re74#64, CASE WHEN (re74#8 = cast(0 as double)) THEN 1 ELSE 0 END AS u74#80]\n                                                                  +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, educ2#49, (educ#3 * re74#8) AS educ_re74#64]\n                                                                     +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, age3#35, POWER(educ#3, cast(2 as double)) AS educ2#49]\n                                                                        +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, age2#22, POWER(age#2, cast(3 as double)) AS age3#35]\n                                                                           +- Project [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10, POWER(age#2, cast(2 as double)) AS age2#22]\n                                                                              +- LogicalRDD [index#0L, treat#1, age#2, educ#3, black#4, hisp#5, marr#6, nodegree#7, re74#8, re75#9, re78#10], false\n"
     ]
    }
   ],
   "source": [
    "r.select('id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-----------+----------+---------------------+\n",
      "| id|            features|       rawPrediction|probability|prediction|marginal_contribution|\n",
      "+---+--------------------+--------------------+-----------+----------+---------------------+\n",
      "|  3|[3.0,2304.0,11059...|[1.54350200272500...|0.043646522|       0.0|                  0.0|\n",
      "+---+--------------------+--------------------+-----------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r.filter('id=3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3314"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted.select('index').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021847155001780854"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.select(F.sum('shap')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041614942312058444\n",
      "0.043646521866321564\n"
     ]
    }
   ],
   "source": [
    "print(df_predicted.select('probability').toPandas().probability.mean() + a.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04829068469926582\n",
      "0.04364077374339104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(df_predicted.select('probability').toPandas().probability.mean() + a.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052278824448785434\n",
      "0.043646443635225296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(df_predicted.select('probability').toPandas().probability.mean() + a.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043646443635225296"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
