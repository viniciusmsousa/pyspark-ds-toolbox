{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/07.000504/Documents/poetry-envs/pyspark-ds-toolbox-Fn-Rjt-3-py3.7/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType, StructField, StructType, StringType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "import pyspark.ml.feature as FF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_ds_toolbox.ml.eval import get_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/03 17:52:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Ml-Pipes') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file): \n",
    "    return pd.read_stata(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n",
    "\n",
    "df = read_data('nsw_mixtape.dta')\n",
    "df = pd.concat((df, read_data('cps_mixtape.dta')))\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = spark.createDataFrame(df.drop(columns=['data_id']))\\\n",
    "    .withColumn('age2', F.col('age')**2)\\\n",
    "    .withColumn('age3', F.col('age')**3)\\\n",
    "    .withColumn('educ2', F.col('educ')**2)\\\n",
    "    .withColumn('educ_re74', F.col('educ')*F.col('re74'))\\\n",
    "    .withColumn('u74', F.when(F.col('re74')==0, 1).otherwise(0))\\\n",
    "    .withColumn('u75', F.when(F.col('re75')==0, 1).otherwise(0))\n",
    "\n",
    "features=['age', 'age2', 'age3', 'educ', 'educ2', 'marr', 'nodegree', 'black', 'hisp', 're74', 're75', 'u74', 'u75', 'educ_re74']\n",
    "assembler = FF.VectorAssembler(inputCols=features, outputCol='features')\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "df_assembled = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- treat: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- educ: double (nullable = true)\n",
      " |-- black: double (nullable = true)\n",
      " |-- hisp: double (nullable = true)\n",
      " |-- marr: double (nullable = true)\n",
      " |-- nodegree: double (nullable = true)\n",
      " |-- re74: double (nullable = true)\n",
      " |-- re75: double (nullable = true)\n",
      " |-- re78: double (nullable = true)\n",
      " |-- age2: double (nullable = true)\n",
      " |-- age3: double (nullable = true)\n",
      " |-- educ2: double (nullable = true)\n",
      " |-- educ_re74: double (nullable = true)\n",
      " |-- u74: integer (nullable = false)\n",
      " |-- u75: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: float (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_size=0.8\n",
    "train, test = df_assembled.randomSplit([train_size, (1-train_size)], seed=12345)\n",
    "\n",
    "model = GBTClassifier(labelCol='treat')\n",
    "p = Pipeline(stages=[model])\n",
    "p_fitted = p.fit(train)\n",
    "\n",
    "df_predicted = p_fitted.transform(test).withColumn('probability', get_p1(F.col('probability')))\n",
    "\n",
    "df_predicted.printSchema()\n",
    "v = df_predicted.filter(F.col('index')==3).select('probability').collect()[0][0]\n",
    "m = df_predicted.select('probability').toPandas().probability.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from psutil import virtual_memory\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F, SparkSession, types as T, Window\n",
    "\n",
    "import operator\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField('id', IntegerType(), True),\n",
    "  StructField('feature', StringType(), True),\n",
    "  StructField('shap', FloatType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_permutations(\n",
    "        df,\n",
    "        feature_names,\n",
    "        output_col='features_permutations'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a column for the ordered features and then shuffles it.\n",
    "    The result is a dataframe with a column `output_col` that contains:\n",
    "    [feat2, feat4, feat3, feat1],\n",
    "    [feat3, feat4, feat2, feat1],\n",
    "    [feat1, feat2, feat4, feat3],\n",
    "    ...\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        output_col,\n",
    "        F.shuffle(\n",
    "            F.array(*[F.lit(f) for f in feature_names])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_shapley_values(\n",
    "        df,\n",
    "        model,\n",
    "        row_of_interest,\n",
    "        feature_names,\n",
    "        features_col='features',\n",
    "        column_to_examine='anomalyScore'\n",
    "):\n",
    "    \"\"\"\n",
    "    # Based on the algorithm described here:\n",
    "    # https://christophm.github.io/interpretable-ml-book/shapley.html#estimating-the-shapley-value\n",
    "    # And on Baskerville's implementation for IForest/ AnomalyModel here:\n",
    "    # https://github.com/equalitie/baskerville/blob/develop/src/baskerville/util/model_interpretation/helpers.py#L235\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField('id', IntegerType(), True),\n",
    "        StructField('feature', StringType(), True),\n",
    "        StructField('shap', FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    results = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n",
    "\n",
    "    features_perm_col = 'features_permutations'\n",
    "    # spark = get_spark_session()\n",
    "    marginal_contribution_filter = F.avg('marginal_contribution').alias('shap_value')\n",
    "    # broadcast the row of interest and ordered feature names\n",
    "    ROW_OF_INTEREST_BROADCAST = spark.sparkContext.broadcast(row_of_interest)\n",
    "    ORDERED_FEATURE_NAMES = spark.sparkContext.broadcast(feature_names)\n",
    "\n",
    "    # persist before continuing with calculations\n",
    "    if not df.is_cached:\n",
    "        df = df.persist()\n",
    "\n",
    "    # get permutations\n",
    "    features_df = get_features_permutations(\n",
    "        df,\n",
    "        feature_names,\n",
    "        output_col=features_perm_col\n",
    "    )\n",
    "\n",
    "    # set up the udf - x-j and x+j need to be calculated for every row\n",
    "    def calculate_x(\n",
    "            feature_j, z_features, curr_feature_perm\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The instance  x+j is the instance of interest,\n",
    "        but all values in the order before feature j are\n",
    "        replaced by feature values from the sample z\n",
    "        The instance  xâˆ’j is the same as  x+j, but in addition\n",
    "        has feature j replaced by the value for feature j from the sample z\n",
    "        \"\"\"\n",
    "        x_interest = ROW_OF_INTEREST_BROADCAST.value\n",
    "        ordered_features = ORDERED_FEATURE_NAMES.value\n",
    "        x_minus_j = list(z_features).copy()\n",
    "        x_plus_j = list(z_features).copy()\n",
    "        f_i = curr_feature_perm.index(feature_j)\n",
    "        after_j = False\n",
    "        for f in curr_feature_perm[f_i:]:\n",
    "            # replace z feature values with x of interest feature values\n",
    "            # iterate features in current permutation until one before j\n",
    "            # x-j = [z1, z2, ... zj-1, xj, xj+1, ..., xN]\n",
    "            # we already have zs because we go row by row with the udf,\n",
    "            # so replace z_features with x of interest\n",
    "            f_index = ordered_features.index(f)\n",
    "            new_value = x_interest[f_index]\n",
    "            x_plus_j[f_index] = new_value\n",
    "            if after_j:\n",
    "                x_minus_j[f_index] = new_value\n",
    "            after_j = True\n",
    "\n",
    "        # minus must be first because of lag\n",
    "        return Vectors.dense(x_minus_j), Vectors.dense(x_plus_j)\n",
    "\n",
    "    udf_calculate_x = F.udf(calculate_x, T.ArrayType(VectorUDT()))\n",
    "\n",
    "    # persist before processing\n",
    "    features_df = features_df.persist()\n",
    "\n",
    "    for f in feature_names:\n",
    "        # x column contains x-j and x+j in this order.\n",
    "        # Because lag is calculated this way:\n",
    "        # F.col('anomalyScore') - (F.col('anomalyScore') one row before)\n",
    "        # x-j needs to be first in `x` column array so we should have:\n",
    "        # id1, [x-j row i,  x+j row i]\n",
    "        # ...\n",
    "        # that with explode becomes:\n",
    "        # id1, x-j row i\n",
    "        # id1, x+j row i\n",
    "        # ...\n",
    "        # to give us (x+j - x-j) when we calculate marginal contribution\n",
    "        # Note that with explode, x-j and x+j for the same row have the same id\n",
    "        # This gives us the opportunity to use lag with\n",
    "        # a window partitioned by id\n",
    "        x_df = features_df.withColumn('x', udf_calculate_x(\n",
    "            F.lit(f), features_col, features_perm_col\n",
    "        )).persist()\n",
    "\n",
    "        # Calculating SHAP values for f\n",
    "        x_df = x_df.selectExpr(\n",
    "            'id', f'explode(x) as {features_col}'\n",
    "        ).cache()\n",
    "        x_df = model.transform(x_df).withColumn('probability', get_p1(F.col('probability')))\n",
    "\n",
    "        # marginal contribution is calculated using a window and a lag of 1.\n",
    "        # the window is partitioned by id because x+j and x-j for the same row\n",
    "        # will have the same id\n",
    "        x_df = x_df.withColumn(\n",
    "            'marginal_contribution',\n",
    "            F.col(column_to_examine) - F.lag(F.col(column_to_examine), 1).over(Window.partitionBy('id').orderBy('id'))\n",
    "        )\n",
    "        # calculate the average\n",
    "        x_df = x_df.filter(x_df.marginal_contribution.isNotNull())\n",
    "        \n",
    "        temp = pd.DataFrame.from_dict({\n",
    "            'id': [row_of_interest['id']],\n",
    "            'feature': [f],\n",
    "            'shap_value': [x_df.select(marginal_contribution_filter).first().shap_value]\n",
    "        })\n",
    "        temp = spark.createDataFrame(temp)\n",
    "        # results[f] = x_df.select(\n",
    "        #     marginal_contribution_filter\n",
    "        # ).first().shap_value\n",
    "        # x_df.unpersist()\n",
    "        print(f'Marginal Contribution for feature: {f} = {x_df.select(marginal_contribution_filter).first().shap_value}')\n",
    "        # break\n",
    "        results = results.union(temp)\n",
    "        #del x_df\n",
    " \n",
    "    # ordered_results = sorted(\n",
    "    #     results.items(),\n",
    "    #     key=operator.itemgetter(1),\n",
    "    #     reverse=True\n",
    "    # )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: age = 0.0036901990057246347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: age2 = 0.0005360889033629344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: age3 = 0.00030300699254374596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: educ = 0.009800358720959015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: educ2 = 0.0004174735999100252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: marr = -0.0029881995673345253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: nodegree = 0.000807448134790344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: black = -0.004834027502177084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: hisp = 0.004992575887268543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: re74 = -0.009940294177895085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: re75 = -0.016294625012973768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: u74 = 0.006179875643123063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: u75 = 0.0025265846272611732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Contribution for feature: educ_re74 = -0.000784117700148887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = calculate_shapley_values(\n",
    "    df = df_predicted.withColumnRenamed('index', 'id'),\n",
    "    model = p_fitted,\n",
    "    row_of_interest = df_predicted.filter(F.col('index')==3).withColumnRenamed('index', 'id').first(),\n",
    "    feature_names = features,\n",
    "    features_col='features',\n",
    "    column_to_examine='probability'\n",
    ")\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|feature|                shap|\n",
      "+---+-------+--------------------+\n",
      "|  3|    age|0.003690199005724...|\n",
      "|  3|   age2|5.360889033629344E-4|\n",
      "|  3|   age3|3.030069925437459...|\n",
      "|  3|   educ|0.009800358720959015|\n",
      "|  3|  educ2|4.174735999100252E-4|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04829068469926582\n",
      "0.04364077374339104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(df_predicted.select('probability').toPandas().probability.mean() + a.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052278824448785434\n",
      "0.043646443635225296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(df_predicted.select('probability').toPandas().probability.mean() + a.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043646443635225296"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
