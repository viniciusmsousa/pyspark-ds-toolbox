{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Value as Feature Selection for Binary Classification Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark_ds_toolbox.ml.data_prep.features_vector import get_features_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/29 14:18:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Spark-Toolbox') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lendo o dataset base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|\n",
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "|    0|37.0|11.0|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|    3|48.0| 6.0|                CPS1| marr|       b|    0|\n",
      "|    7|18.0|11.0|                CPS1|other|       b|    0|\n",
      "|   10|19.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|   12|18.0| 8.0|Dehejia-Wahba Sample|black|       a|    1|\n",
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(file): \n",
    "    return pd.read_stata(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n",
    "\n",
    "df = read_data('nsw_mixtape.dta')\n",
    "df = pd.concat((df, read_data('cps_mixtape.dta')))\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = spark.createDataFrame(df)\\\n",
    "    .withColumn('etnia', F.expr('case when black=1 then \"black\" when hisp=1 then \"hisp\" when marr=1 then \"marr\" else \"other\" end'))\\\n",
    "    .withColumn('treat', F.col('treat').cast('int'))\\\n",
    "    .withColumn('dumb_cat', F.expr('case when index > 10 then \"a\" else \"b\" end'))\\\n",
    "    .select('index', 'age', 'educ', 'data_id', 'etnia','dumb_cat', 'treat')\n",
    "\n",
    "\n",
    "\n",
    "dfs_train, dfs_test = df.randomSplit([0.8, 0.2], seed=4)\n",
    "dfs_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['age', 'educ']\n",
    "cat_features = ['data_id', 'etnia', 'dumb_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(dfs, feature, target):\n",
    "    # testar se o tipo da coluna target é int e os valores unicos são 0 e 1\n",
    "    cross = dfs\\\n",
    "        .crosstab(feature, target)\\\n",
    "        .withColumnRenamed(f'{feature}_{target}', 'feature_value')\n",
    "        \n",
    "    sum_0 = cross.select('0').groupBy().sum().collect()[0][0]\n",
    "    sum_1 = cross.select('1').groupBy().sum().collect()[0][0]\n",
    "\n",
    "    \n",
    "    cross = cross\\\n",
    "        .withColumn('0', F.col('0')/sum_0)\\\n",
    "        .withColumn('1', F.col('1')/sum_1)\\\n",
    "        .withColumn('woe', F.log(F.col('0')/F.col('1')))\\\n",
    "        .withColumn('iv', F.col('woe')*(F.col('0') - F.col('1')))\\\n",
    "        .withColumn('feature', F.lit(feature))\\\n",
    "        .fillna(0)\n",
    "\n",
    "\n",
    "    iv = cross.selectExpr('sum(iv) as iv').collect()[0][0]  \n",
    "    \n",
    "    \n",
    "    return cross.select('feature', 'feature_value', 'woe', 'iv'), iv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = calculate_woe_iv(dfs=dfs_test,feature='dumb_cat',target='treat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+--------------------+\n",
      "| feature|feature_value|                woe|                  iv|\n",
      "+--------+-------------+-------------------+--------------------+\n",
      "|dumb_cat|            a|0.04818133614453751|0.002265024120408...|\n",
      "|dumb_cat|            b| -4.359756680313845| 0.20495392677358656|\n",
      "+--------+-------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20721895089399464"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "\n",
    "def feature_selection_iv(dfs, target, num_features, cat_features, floor_iv=0.3, floor_bucket_percent=0.1, categorical_as_woe=False):\n",
    "    # msm teste de num_features e cat_features da get_featues_vector\n",
    "\n",
    "    schema_woe = StructType([\n",
    "        StructField(\"feature\", StringType(), False),\n",
    "        StructField(\"feature_value\", StringType(), True),\n",
    "        StructField(\"woe\", FloatType(), True),\n",
    "        StructField(\"iv\", FloatType(), True)\n",
    "    ])\n",
    "    dfs_woe = spark.createDataFrame([], schema_woe)\n",
    "\n",
    "\n",
    "    schema_iv = StructType([\n",
    "        StructField('feature', StringType(), False),\n",
    "        StructField('iv', FloatType(), False)\n",
    "    ])\n",
    "    dfs_iv = spark.createDataFrame([], schema_iv)\n",
    "\n",
    "\n",
    "    if num_features is not None:\n",
    "        count_dfs = dfs.count()\n",
    "        nBuckets = count_dfs/(count_dfs*floor_bucket_percent)\n",
    "\n",
    "        bucket_num_features = [i + '_bucket' for i in num_features]\n",
    "        qt = QuantileDiscretizer(inputCols=num_features, outputCols=bucket_num_features, numBuckets=nBuckets)\n",
    "        dfs = qt.fit(dfs).transform(dfs)\n",
    "\n",
    "    \n",
    "    # criar aqui logica tbm\n",
    "    feats = bucket_num_features + cat_features\n",
    "\n",
    "    for f in feats:\n",
    "        df_woe_feature, iv = calculate_woe_iv(dfs=dfs,feature=f,target=target)\n",
    "    \n",
    "        dfs_woe = dfs_woe.union(df_woe_feature)\n",
    "        dfs_iv = dfs_iv.union(spark.createDataFrame(pd.DataFrame({'feature':[f],'iv':[iv]})))\n",
    "\n",
    "    \n",
    "    cols_to_keep = dfs_iv.filter(f'iv >= {floor_iv}').toPandas()['feature'].to_list()\n",
    "\n",
    "    if categorical_as_woe:\n",
    "        selected_features = [s[:-7] if s.endswith('_bucket') else s+'_woe' for s in cols_to_keep]\n",
    "        \n",
    "        stages_features_vector = [WeightOfEvidenceComputer(inputCols=cat_features, target=target)] + get_features_vector(num_features=selected_features)\n",
    "    else:\n",
    "        num_selectec_features = list(filter(None, [s[:-6] if s.endswith('_bucket') else None for s in cols_to_keep]))\n",
    "        cat_selectec_features = list(filter(None, [None if s.endswith('_bucket') else s for s in cols_to_keep]))\n",
    "        \n",
    "        stages_features_vector = get_features_vector(num_features=num_selectec_features, cat_features=cat_selectec_features)\n",
    "\n",
    "    out_dict = {\n",
    "        'dfs_woe': dfs_woe,\n",
    "        'dfs_iv': dfs_iv.orderBy(F.col('iv').desc()),\n",
    "        'stages_features_vector': stages_features_vector\n",
    "    }\n",
    "\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feature_selection_iv(dfs=dfs_train, target='treat', num_features=num_features, cat_features=cat_features, categorical_as_woe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dfs_woe', 'dfs_iv', 'stages_features_vector'])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WeightOfEvidenceComputer_7434319b69e4,\n",
       " VectorAssembler_ad01e946afa1,\n",
       " VectorAssembler_d5038197cf6e]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['stages_features_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+--------------------+\n",
      "|   feature|feature_value|                 woe|                  iv|\n",
      "+----------+-------------+--------------------+--------------------+\n",
      "|age_bucket|          0.0| -0.5972201927175608| 0.03755893994515182|\n",
      "|age_bucket|          5.0|  0.4674214381624044|0.019477216762649557|\n",
      "|age_bucket|          1.0|-0.42042489967372465|0.020182294438309992|\n",
      "|age_bucket|          6.0|  0.9472248535629343| 0.05228097899890407|\n",
      "|age_bucket|          9.0|                 0.0|                 0.0|\n",
      "+----------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out['dfs_woe'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    feature|                iv|\n",
      "+-----------+------------------+\n",
      "|    data_id| 4.081508165850559|\n",
      "|      etnia| 4.076126036555841|\n",
      "|educ_bucket|1.0052276924505816|\n",
      "| age_bucket|0.4417967451554041|\n",
      "|   dumb_cat|0.2845468150802483|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out['dfs_iv'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline(stages=out['stages_features_vector'])\n",
    "fitted = p.fit(dfs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|       data_id_woe|          etnia_woe|       dumb_cat_woe|                 num|            features|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|    0|37.0|11.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|[37.0,11.0,-4.090...|[37.0,11.0,-4.090...|\n",
      "|    3|48.0| 6.0|                CPS1| marr|       b|    0|               0.0|  3.234624562237972| -4.359756680313845|[48.0,6.0,0.0,3.2...|[48.0,6.0,0.0,3.2...|\n",
      "|    7|18.0|11.0|                CPS1|other|       b|    0|               0.0| 1.2022061808577313| -4.359756680313845|[18.0,11.0,0.0,1....|[18.0,11.0,0.0,1....|\n",
      "|   10|19.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|[19.0,9.0,-4.0900...|[19.0,9.0,-4.0900...|\n",
      "|   12|18.0| 8.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[18.0,8.0,-4.0900...|[18.0,8.0,-4.0900...|\n",
      "|   13|19.0|12.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|[19.0,12.0,0.0,1....|[19.0,12.0,0.0,1....|\n",
      "|   16|27.0|12.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|[27.0,12.0,0.0,3....|[27.0,12.0,0.0,3....|\n",
      "|   16|27.0|13.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[27.0,13.0,-4.090...|[27.0,13.0,-4.090...|\n",
      "|   17|23.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,10.0,-4.090...|[23.0,10.0,-4.090...|\n",
      "|   20|23.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,11.0,-4.090...|[23.0,11.0,-4.090...|\n",
      "|   22|38.0| 9.0|Dehejia-Wahba Sample|other|       a|    1|-4.090093113364742| 1.2022061808577313|0.04818133614453751|[38.0,9.0,-4.0900...|[38.0,9.0,-4.0900...|\n",
      "|   23|28.0|16.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|[28.0,16.0,0.0,3....|[28.0,16.0,0.0,3....|\n",
      "|   29|24.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[24.0,11.0,-4.090...|[24.0,11.0,-4.090...|\n",
      "|   30|17.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[17.0,10.0,-4.090...|[17.0,10.0,-4.090...|\n",
      "|   31|48.0| 4.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[48.0,4.0,-4.0900...|[48.0,4.0,-4.0900...|\n",
      "|   36|21.0|13.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|[21.0,13.0,0.0,1....|[21.0,13.0,0.0,1....|\n",
      "|   37|23.0|12.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,12.0,-4.090...|[23.0,12.0,-4.090...|\n",
      "|   44|20.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[20.0,11.0,-4.090...|[20.0,11.0,-4.090...|\n",
      "|   44|42.0|12.0|                CPS1| hisp|       a|    0|               0.0| 1.1208822430281464|0.04818133614453751|[42.0,12.0,0.0,1....|[42.0,12.0,0.0,1....|\n",
      "|   45|25.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[25.0,11.0,-4.090...|[25.0,11.0,-4.090...|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = fitted.transform(dfs_test)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.save(\"tmp/food_pipeline.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted2 = PipelineModel.read().load(\"tmp/food_pipeline.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|       data_id_woe|          etnia_woe|       dumb_cat_woe|                 num|            features|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|    0|37.0|11.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|[37.0,11.0,-4.090...|[37.0,11.0,-4.090...|\n",
      "|    3|48.0| 6.0|                CPS1| marr|       b|    0|               0.0|  3.234624562237972| -4.359756680313845|[48.0,6.0,0.0,3.2...|[48.0,6.0,0.0,3.2...|\n",
      "|    7|18.0|11.0|                CPS1|other|       b|    0|               0.0| 1.2022061808577313| -4.359756680313845|[18.0,11.0,0.0,1....|[18.0,11.0,0.0,1....|\n",
      "|   10|19.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|[19.0,9.0,-4.0900...|[19.0,9.0,-4.0900...|\n",
      "|   12|18.0| 8.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[18.0,8.0,-4.0900...|[18.0,8.0,-4.0900...|\n",
      "|   13|19.0|12.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|[19.0,12.0,0.0,1....|[19.0,12.0,0.0,1....|\n",
      "|   16|27.0|12.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|[27.0,12.0,0.0,3....|[27.0,12.0,0.0,3....|\n",
      "|   16|27.0|13.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[27.0,13.0,-4.090...|[27.0,13.0,-4.090...|\n",
      "|   17|23.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,10.0,-4.090...|[23.0,10.0,-4.090...|\n",
      "|   20|23.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,11.0,-4.090...|[23.0,11.0,-4.090...|\n",
      "|   22|38.0| 9.0|Dehejia-Wahba Sample|other|       a|    1|-4.090093113364742| 1.2022061808577313|0.04818133614453751|[38.0,9.0,-4.0900...|[38.0,9.0,-4.0900...|\n",
      "|   23|28.0|16.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|[28.0,16.0,0.0,3....|[28.0,16.0,0.0,3....|\n",
      "|   29|24.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[24.0,11.0,-4.090...|[24.0,11.0,-4.090...|\n",
      "|   30|17.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[17.0,10.0,-4.090...|[17.0,10.0,-4.090...|\n",
      "|   31|48.0| 4.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[48.0,4.0,-4.0900...|[48.0,4.0,-4.0900...|\n",
      "|   36|21.0|13.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|[21.0,13.0,0.0,1....|[21.0,13.0,0.0,1....|\n",
      "|   37|23.0|12.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[23.0,12.0,-4.090...|[23.0,12.0,-4.090...|\n",
      "|   44|20.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[20.0,11.0,-4.090...|[20.0,11.0,-4.090...|\n",
      "|   44|42.0|12.0|                CPS1| hisp|       a|    0|               0.0| 1.1208822430281464|0.04818133614453751|[42.0,12.0,0.0,1....|[42.0,12.0,0.0,1....|\n",
      "|   45|25.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|[25.0,11.0,-4.090...|[25.0,11.0,-4.090...|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = fitted2.transform(dfs_test)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wip Transformer WOE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_woe(dfs, feature, target):\n",
    "    dfs_woe_feature, iv = calculate_woe_iv(dfs=dfs,feature=feature,target=target)\n",
    "    dfs_woe_feature = dfs_woe_feature\\\n",
    "        .withColumnRenamed('feature_value', feature)\\\n",
    "        .withColumnRenamed('woe', f'{feature}_woe')\\\n",
    "        .select(feature, f'{feature}_woe')\n",
    "\n",
    "\n",
    "    cols = dfs.columns + [f'{feature}_woe']\n",
    "    dfs = dfs.join(dfs_woe_feature, on=feature, how='left').select(cols)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+-------------------+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|        data_id_woe|\n",
      "+-----+----+----+--------------------+-----+--------+-----+-------------------+\n",
      "|    0|45.0|11.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    1|21.0|14.0|                CPS1|other|       b|    0|                0.0|\n",
      "|    1|22.0| 9.0|Dehejia-Wahba Sample| hisp|       b|    1|-4.1470758465965325|\n",
      "|    2|30.0|12.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    2|38.0|12.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    3|27.0|11.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    4|18.0| 8.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    4|33.0| 8.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    5|22.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    5|22.0|11.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    6|23.0|12.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    6|48.0|10.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    7|32.0|11.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    8|22.0|16.0|Dehejia-Wahba Sample|black|       b|    1|-4.1470758465965325|\n",
      "|    8|48.0| 9.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|    9|33.0|12.0|Dehejia-Wahba Sample| marr|       b|    1|-4.1470758465965325|\n",
      "|    9|45.0|12.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|   10|34.0|14.0|                CPS1| marr|       b|    0|                0.0|\n",
      "|   11|16.0|10.0|                CPS1|other|       a|    0|                0.0|\n",
      "|   11|21.0|13.0|Dehejia-Wahba Sample|black|       a|    1|-4.1470758465965325|\n",
      "+-----+----+----+--------------------+-----+--------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_woe(dfs=dfs_train, feature='data_id', target='treat').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class WeightOfEvidenceComputer(Transformer, HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    target = Param(\n",
    "        parent=Params._dummy(),\n",
    "        name='target',\n",
    "        doc='Column name of the target. Must be a integer os values 0 or 1.',\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputCol=None,\n",
    "        # outputCol=None,\n",
    "        inputCols=None,\n",
    "        # outputCols=None,\n",
    "        target=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(target=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(\n",
    "        self,\n",
    "        inputCol=None,\n",
    "        # outputCol=None,\n",
    "        inputCols=None,\n",
    "        # outputCols=None,\n",
    "        target=None,\n",
    "    ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setTarget(self, new_target):\n",
    "        return self.setParams(target=new_target)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    # def setOutputCol(self, new_outputCol):\n",
    "    #     return self.setParams(outputCol=new_outputCol)\n",
    "\n",
    "    def setInputCols(self, new_inputCols):\n",
    "        return self.setParams(inputCols=new_inputCols)\n",
    "\n",
    "    # def setOutputCols(self, new_outputCols):\n",
    "    #     return self.setParams(outputCols=new_outputCols)\n",
    "\n",
    "    def getTarget(self):\n",
    "        return self.getOrDefault(self.target)\n",
    "    \n",
    "    def checkParams(self):\n",
    "        # Test #1: either inputCol or inputCols can be set (but not both).\n",
    "        if self.isSet(\"inputCol\") and (self.isSet(\"inputCols\")):\n",
    "            raise ValueError(\n",
    "                \"Only one of `inputCol` and `inputCols`\" \"must be set.\"\n",
    "            )\n",
    "\n",
    "        # Test #2: at least one of inputCol or inputCols must be set.\n",
    "        if not (self.isSet(\"inputCol\") or self.isSet(\"inputCols\")):\n",
    "            raise ValueError(\n",
    "                \"One of `inputCol` or `inputCols` must be set.\"\n",
    "            )\n",
    "\n",
    "        # Test #3: if `inputCols` is set, then `outputCols`\n",
    "        # must be a list of the same len()\n",
    "        # if self.isSet(\"inputCols\"):\n",
    "        #     if len(self.getInputCols()) != len(self.getOutputCols()):\n",
    "        #         raise ValueError(\n",
    "        #             \"The length of `inputCols` does not match\"\n",
    "        #             \" the length of `outputCols`\"\n",
    "        #         )\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        self.checkParams()\n",
    "\n",
    "        # If `inputCol` / `outputCol`, we wrap into a single-item list\n",
    "        input_columns = (\n",
    "            [self.getInputCol()]\n",
    "            if self.isSet(\"inputCol\")\n",
    "            else self.getInputCols()\n",
    "        )\n",
    "        # output_columns = (\n",
    "        #     [self.getOutputCol()]\n",
    "        #     if self.isSet(\"outputCol\")\n",
    "        #     else self.getOutputCols()\n",
    "        # )\n",
    "\n",
    "        # answer = dataset\n",
    "\n",
    "        for feat in input_columns:\n",
    "            dataset = add_woe(dfs=dataset, feature=feat, target=self.getTarget())\n",
    "        # # If input_columns == output_columns, we overwrite and no need to create new columns.\n",
    "        # if input_columns != output_columns:\n",
    "        #     for in_col, out_col in zip(input_columns, output_columns):\n",
    "        #         answer = answer.withColumn(out_col, F.col(in_col))\n",
    "\n",
    "                \n",
    "\n",
    "        # na_filler = self.getFiller()\n",
    "        return dataset#.fillna(na_filler, output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|       data_id_woe|          etnia_woe|       dumb_cat_woe|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+\n",
      "|    0|37.0|11.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|\n",
      "|    3|48.0| 6.0|                CPS1| marr|       b|    0|               0.0|  3.234624562237972| -4.359756680313845|\n",
      "|    7|18.0|11.0|                CPS1|other|       b|    0|               0.0| 1.2022061808577313| -4.359756680313845|\n",
      "|   10|19.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|-4.090093113364742|-2.3430534792674322| -4.359756680313845|\n",
      "|   12|18.0| 8.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   13|19.0|12.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|\n",
      "|   16|27.0|12.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|\n",
      "|   16|27.0|13.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   17|23.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   20|23.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   22|38.0| 9.0|Dehejia-Wahba Sample|other|       a|    1|-4.090093113364742| 1.2022061808577313|0.04818133614453751|\n",
      "|   23|28.0|16.0|                CPS1| marr|       a|    0|               0.0|  3.234624562237972|0.04818133614453751|\n",
      "|   29|24.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   30|17.0|10.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   31|48.0| 4.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   36|21.0|13.0|                CPS1|other|       a|    0|               0.0| 1.2022061808577313|0.04818133614453751|\n",
      "|   37|23.0|12.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   44|20.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "|   44|42.0|12.0|                CPS1| hisp|       a|    0|               0.0| 1.1208822430281464|0.04818133614453751|\n",
      "|   45|25.0|11.0|Dehejia-Wahba Sample|black|       a|    1|-4.090093113364742|-2.3430534792674322|0.04818133614453751|\n",
      "+-----+----+----+--------------------+-----+--------+-----+------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_woe = WeightOfEvidenceComputer(inputCols=cat_features, target='treat')\n",
    "\n",
    "test_woe.transform(dfs_test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|   feature|       feature_value|                 woe|                  iv|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "| age_quant|                 0.0| -0.5972201927175608| 0.03755893994515182|\n",
      "| age_quant|                 5.0|  0.4674214381624044|0.019477216762649557|\n",
      "| age_quant|                 1.0|-0.42042489967372465|0.020182294438309992|\n",
      "| age_quant|                 6.0|  0.9472248535629343| 0.05228097899890407|\n",
      "| age_quant|                 9.0|                 0.0|                 0.0|\n",
      "| age_quant|                 2.0| -0.5239091623421057|0.034363226987371216|\n",
      "| age_quant|                 7.0|  0.5402080717439384| 0.02164954344664573|\n",
      "| age_quant|                 3.0| -0.8934823718292055|  0.1402662918441354|\n",
      "| age_quant|                 8.0|  1.3837121700365596|  0.1157164450269391|\n",
      "| age_quant|                 4.0| 0.05293162458232201|3.018077052971832...|\n",
      "|educ_quant|                 0.0| 0.25350722233150136|0.003069068548120...|\n",
      "|educ_quant|                 5.0|  0.4567682572811666|0.011095883411159808|\n",
      "|educ_quant|                 1.0| -0.9409640634534296| 0.15647768048594535|\n",
      "|educ_quant|                 6.0|  0.8688085196133551|0.050454150311016764|\n",
      "|educ_quant|                 2.0| -0.8277809632775891| 0.07821412146105852|\n",
      "|educ_quant|                 7.0|   3.004283452915832|  0.4027789652168664|\n",
      "|educ_quant|                 3.0| -1.1728070299725608|  0.1812199892743036|\n",
      "|educ_quant|                 4.0|  0.6528044970065449| 0.12191783374211053|\n",
      "|   data_id|Dehejia-Wahba Sample| -4.1470758465965325|   4.081508165850559|\n",
      "|   data_id|                CPS1|                 0.0|                 0.0|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|   feature|                iv|\n",
      "+----------+------------------+\n",
      "| age_quant|0.4417967451554041|\n",
      "|educ_quant|1.0052276924505816|\n",
      "|   data_id| 4.081508165850559|\n",
      "|     etnia| 4.076126036555841|\n",
      "|  dumb_cat|0.2845468150802483|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_iv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age_quant', 'educ_quant', 'data_id', 'etnia']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'educ', 'data_id_woe', 'etnia_woe']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical as woe True\n",
    "[s[:-6] if s.endswith('_quant') else s+'_woe' for s in cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'educ']\n",
      "['data_id', 'etnia']\n"
     ]
    }
   ],
   "source": [
    "# categorical as woe False\n",
    "num_selectec_features = list(filter(None, [s[:-6] if s.endswith('_quant') else None for s in cols_to_keep]))\n",
    "\n",
    "cat_selectec_features = list(filter(None, [None if s.endswith('_quant') else s for s in cols_to_keep]))\n",
    "\n",
    "print(num_selectec_features)\n",
    "print(cat_selectec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+--------+-----+\n",
      "|index|             data_id|etnia|dumb_cat|treat|\n",
      "+-----+--------------------+-----+--------+-----+\n",
      "|    0|                CPS1| marr|       b|    0|\n",
      "|    3|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|    7|                CPS1|other|       b|    0|\n",
      "|   10|                CPS1| marr|       b|    0|\n",
      "|   12|                CPS1| marr|       a|    0|\n",
      "+-----+--------------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "| feature|       feature_value|                 woe|                  iv|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "| data_id|Dehejia-Wahba Sample| -4.0928282617041365|   4.024510946258392|\n",
      "| data_id|                CPS1|                 0.0|                 0.0|\n",
      "|   etnia|               other|  0.4487629426410833|0.038510203869811445|\n",
      "|   etnia|               black|  -2.254043853831219|  1.6506230367881973|\n",
      "|   etnia|                marr|   2.994244065051656|  1.7212600705769427|\n",
      "|   etnia|                hisp|                 0.0|                 0.0|\n",
      "|dumb_cat|                   a|0.029860773527106543|8.776845743563007E-4|\n",
      "|dumb_cat|                   b| -3.5050415968020174| 0.10302214506257643|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"A\", \"B\").pivot(\"C\").sum(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "| feature|                 iv|\n",
      "+--------+-------------------+\n",
      "| data_id|  4.024510946258392|\n",
      "|   etnia| 3.4103933112349516|\n",
      "|dumb_cat|0.10389982963693273|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.groupBy('feature').agg(F.sum('iv').alias('iv')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+\n",
      "|etnia_treat|   0|  1|\n",
      "+-----------+----+---+\n",
      "|      other| 782|  5|\n",
      "|      black| 283| 27|\n",
      "|       marr|1994|  1|\n",
      "|       hisp| 236|  0|\n",
      "+-----------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_test.select(target).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Baseline Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Features Vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Class Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instanciating Classifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on Test Data and Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.95s/it]                                   \n"
     ]
    }
   ],
   "source": [
    "base_line_out = baseline_binary_classfiers(\n",
    "    dfs=dfs_train,\n",
    "    id_col='index',\n",
    "    target_col='treat',\n",
    "    num_features=['age', 'educ', 'nodegree', 're74', 're75', 're78', 'age2', 'age3', 'educ2', 'educ_re74', 'u74', 'u75'],\n",
    "    cat_features=['data_id', 'etnia'],\n",
    "    dfs_test=dfs_test,\n",
    "    weight_on_target=True,\n",
    "    log_mlflow_run=False,\n",
    "    artifact_stage_path = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GBTClassifier'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_line_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'metrics', 'decile_metrics'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_line_out['LogisticRegression'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confusion_matrix':    treat  prediction  count\n",
       " 0    0.0         0.0   3240\n",
       " 1    1.0         1.0     33\n",
       " 2    0.0         1.0     55,\n",
       " 'accuracy': 0.9834735576923077,\n",
       " 'f1': 0.5454545454545454,\n",
       " 'precision': 0.375,\n",
       " 'recall': 1.0,\n",
       " 'aucroc': 0.9916540212443096,\n",
       " 'aucpr': 0.375}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_line_out['LogisticRegression']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
