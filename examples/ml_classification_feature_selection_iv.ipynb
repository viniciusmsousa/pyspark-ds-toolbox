{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Value as Feature Selection for Binary Classification Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "\n",
    "from pyspark_ds_toolbox.ml.data_prep.features_vector import get_features_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/29 14:18:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Spark-Toolbox') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lendo o dataset base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|\n",
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "|    0|37.0|11.0|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|    3|48.0| 6.0|                CPS1| marr|       b|    0|\n",
      "|    7|18.0|11.0|                CPS1|other|       b|    0|\n",
      "|   10|19.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|   12|18.0| 8.0|Dehejia-Wahba Sample|black|       a|    1|\n",
      "+-----+----+----+--------------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(file): \n",
    "    return pd.read_stata(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n",
    "\n",
    "df = read_data('nsw_mixtape.dta')\n",
    "df = pd.concat((df, read_data('cps_mixtape.dta')))\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = spark.createDataFrame(df)\\\n",
    "    .withColumn('etnia', F.expr('case when black=1 then \"black\" when hisp=1 then \"hisp\" when marr=1 then \"marr\" else \"other\" end'))\\\n",
    "    .withColumn('treat', F.col('treat').cast('int'))\\\n",
    "    .withColumn('dumb_cat', F.expr('case when index > 10 then \"a\" else \"b\" end'))\\\n",
    "    .select('index', 'age', 'educ', 'data_id', 'etnia','dumb_cat', 'treat')\n",
    "\n",
    "\n",
    "\n",
    "dfs_train, dfs_test = df.randomSplit([0.8, 0.2], seed=4)\n",
    "dfs_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['age', 'educ']\n",
    "cat_features = ['data_id', 'etnia', 'dumb_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----+--------+-----+---------+----------+\n",
      "|index| age|educ|             data_id|etnia|dumb_cat|treat|age_quant|educ_quant|\n",
      "+-----+----+----+--------------------+-----+--------+-----+---------+----------+\n",
      "|    0|45.0|11.0|                CPS1| marr|       b|    0|      8.0|       3.0|\n",
      "|    1|21.0|14.0|                CPS1|other|       b|    0|      1.0|       6.0|\n",
      "|    1|22.0| 9.0|Dehejia-Wahba Sample| hisp|       b|    1|      2.0|       1.0|\n",
      "|    2|30.0|12.0|Dehejia-Wahba Sample|black|       b|    1|      4.0|       4.0|\n",
      "|    2|38.0|12.0|                CPS1| marr|       b|    0|      6.0|       4.0|\n",
      "|    3|27.0|11.0|Dehejia-Wahba Sample|black|       b|    1|      3.0|       3.0|\n",
      "|    4|18.0| 8.0|                CPS1| marr|       b|    0|      0.0|       1.0|\n",
      "|    4|33.0| 8.0|Dehejia-Wahba Sample|black|       b|    1|      5.0|       1.0|\n",
      "|    5|22.0| 9.0|Dehejia-Wahba Sample|black|       b|    1|      2.0|       1.0|\n",
      "|    5|22.0|11.0|                CPS1| marr|       b|    0|      2.0|       3.0|\n",
      "|    6|23.0|12.0|Dehejia-Wahba Sample|black|       b|    1|      2.0|       4.0|\n",
      "|    6|48.0|10.0|                CPS1| marr|       b|    0|      8.0|       2.0|\n",
      "|    7|32.0|11.0|Dehejia-Wahba Sample|black|       b|    1|      5.0|       3.0|\n",
      "|    8|22.0|16.0|Dehejia-Wahba Sample|black|       b|    1|      2.0|       7.0|\n",
      "|    8|48.0| 9.0|                CPS1| marr|       b|    0|      8.0|       1.0|\n",
      "|    9|33.0|12.0|Dehejia-Wahba Sample| marr|       b|    1|      5.0|       4.0|\n",
      "|    9|45.0|12.0|                CPS1| marr|       b|    0|      8.0|       4.0|\n",
      "|   10|34.0|14.0|                CPS1| marr|       b|    0|      5.0|       6.0|\n",
      "|   11|16.0|10.0|                CPS1|other|       a|    0|      0.0|       2.0|\n",
      "|   11|21.0|13.0|Dehejia-Wahba Sample|black|       a|    1|      1.0|       5.0|\n",
      "+-----+----+----+--------------------+-----+--------+-----+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(dfs, feature, target):\n",
    "    # testar se o tipo da coluna target é int e os valores unicos são 0 e 1\n",
    "    cross = dfs\\\n",
    "        .crosstab(feature, target)\\\n",
    "        .withColumnRenamed(f'{feature}_{target}', 'feature_value')\n",
    "        \n",
    "    sum_0 = cross.select('0').groupBy().sum().collect()[0][0]\n",
    "    sum_1 = cross.select('1').groupBy().sum().collect()[0][0]\n",
    "\n",
    "    \n",
    "    cross = cross\\\n",
    "        .withColumn('0', F.col('0')/sum_0)\\\n",
    "        .withColumn('1', F.col('1')/sum_1)\\\n",
    "        .withColumn('woe', F.log(F.col('0')/F.col('1')))\\\n",
    "        .withColumn('iv', F.col('woe')*(F.col('0') - F.col('1')))\\\n",
    "        .withColumn('feature', F.lit(feature))\n",
    "\n",
    "\n",
    "    iv = cross.selectExpr('sum(iv) as iv').collect()[0][0]  \n",
    "    \n",
    "    \n",
    "    return cross.select('feature', 'feature_value', 'woe', 'iv'), iv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = calculate_woe_iv(dfs=dfs_test,feature='dumb_cat',target='treat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+--------------------+\n",
      "| feature|feature_value|                woe|                  iv|\n",
      "+--------+-------------+-------------------+--------------------+\n",
      "|dumb_cat|            a|0.04818133614453751|0.002265024120408...|\n",
      "|dumb_cat|            b| -4.359756680313845| 0.20495392677358656|\n",
      "+--------+-------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20721895089399464"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "\n",
    "def feature_selection_iv(dfs, target, num_features, cat_features, floor_iv=0.3, floor_bucket_percent=0.1, categorical_as_woe=False):\n",
    "    # msm teste de num_features e cat_features da get_featues_vector\n",
    "\n",
    "    schema_woe = StructType([\n",
    "        StructField(\"feature\", StringType(), False),\n",
    "        StructField(\"feature_value\", StringType(), True),\n",
    "        StructField(\"woe\", FloatType(), True),\n",
    "        StructField(\"iv\", FloatType(), True)\n",
    "    ])\n",
    "    dfs_woe = spark.createDataFrame([], schema_woe)\n",
    "\n",
    "\n",
    "    schema_iv = StructType([\n",
    "        StructField('feature', StringType(), False),\n",
    "        StructField('iv', FloatType(), False)\n",
    "    ])\n",
    "    dfs_iv = spark.createDataFrame([], schema_iv)\n",
    "\n",
    "\n",
    "    if num_features is not None:\n",
    "        count_dfs = dfs.count()\n",
    "        nBuckets = count_dfs/(count_dfs*floor_bucket_percent)\n",
    "\n",
    "        bucket_num_features = [i + '_bucket' for i in num_features]\n",
    "        qt = QuantileDiscretizer(inputCols=num_features, outputCols=bucket_num_features, numBuckets=nBuckets)\n",
    "        dfs = qt.fit(dfs).transform(dfs)\n",
    "\n",
    "    \n",
    "    # criar aqui logica tbm\n",
    "    feats = bucket_num_features + cat_features\n",
    "\n",
    "    for f in feats:\n",
    "        df_woe_feature, iv = calculate_woe_iv(dfs=dfs,feature=f,target=target)\n",
    "    \n",
    "        dfs_woe = dfs_woe.union(df_woe_feature)\n",
    "        dfs_iv = dfs_iv.union(spark.createDataFrame(pd.DataFrame({'feature':[f],'iv':[iv]})))\n",
    "\n",
    "    dfs_woe = dfs_woe.fillna(0)\n",
    "    cols_to_keep = dfs_iv.filter(f'iv >= {floor_iv}').toPandas()['feature'].to_list()\n",
    "\n",
    "    if categorical_as_woe:\n",
    "        selected_features = [s[:-6] if s.endswith('_bucket') else s+'_woe' for s in cols_to_keep]\n",
    "        \n",
    "        stages_features_vector = get_features_vector(num_features=selected_features)\n",
    "    else:\n",
    "        num_selectec_features = list(filter(None, [s[:-6] if s.endswith('_bucket') else None for s in cols_to_keep]))\n",
    "        cat_selectec_features = list(filter(None, [None if s.endswith('_bucket') else s for s in cols_to_keep]))\n",
    "        \n",
    "        stages_features_vector = get_features_vector(num_features=num_selectec_features, cat_features=cat_selectec_features)\n",
    "\n",
    "\n",
    "    return stages_features_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_4981e2a02208,\n",
       " StringIndexer_d3e376f831d1,\n",
       " OneHotEncoder_1c0e8643899c,\n",
       " OneHotEncoder_8e196dce3c95,\n",
       " VectorAssembler_185b4ea0b1f9,\n",
       " VectorAssembler_6836010161ca,\n",
       " VectorAssembler_aced8b1fc5ba]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_iv(dfs=dfs_test, target='treat', num_features=num_features, cat_features=cat_features, categorical_as_woe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuantileDiscretizer_88984347f251,\n",
       " VectorAssembler_ef92014b7edf,\n",
       " VectorAssembler_89b80508a460]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|   feature|       feature_value|                 woe|                  iv|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "| age_quant|                 0.0| -0.5972201927175608| 0.03755893994515182|\n",
      "| age_quant|                 5.0|  0.4674214381624044|0.019477216762649557|\n",
      "| age_quant|                 1.0|-0.42042489967372465|0.020182294438309992|\n",
      "| age_quant|                 6.0|  0.9472248535629343| 0.05228097899890407|\n",
      "| age_quant|                 9.0|                 0.0|                 0.0|\n",
      "| age_quant|                 2.0| -0.5239091623421057|0.034363226987371216|\n",
      "| age_quant|                 7.0|  0.5402080717439384| 0.02164954344664573|\n",
      "| age_quant|                 3.0| -0.8934823718292055|  0.1402662918441354|\n",
      "| age_quant|                 8.0|  1.3837121700365596|  0.1157164450269391|\n",
      "| age_quant|                 4.0| 0.05293162458232201|3.018077052971832...|\n",
      "|educ_quant|                 0.0| 0.25350722233150136|0.003069068548120...|\n",
      "|educ_quant|                 5.0|  0.4567682572811666|0.011095883411159808|\n",
      "|educ_quant|                 1.0| -0.9409640634534296| 0.15647768048594535|\n",
      "|educ_quant|                 6.0|  0.8688085196133551|0.050454150311016764|\n",
      "|educ_quant|                 2.0| -0.8277809632775891| 0.07821412146105852|\n",
      "|educ_quant|                 7.0|   3.004283452915832|  0.4027789652168664|\n",
      "|educ_quant|                 3.0| -1.1728070299725608|  0.1812199892743036|\n",
      "|educ_quant|                 4.0|  0.6528044970065449| 0.12191783374211053|\n",
      "|   data_id|Dehejia-Wahba Sample| -4.1470758465965325|   4.081508165850559|\n",
      "|   data_id|                CPS1|                 0.0|                 0.0|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|   feature|                iv|\n",
      "+----------+------------------+\n",
      "| age_quant|0.4417967451554041|\n",
      "|educ_quant|1.0052276924505816|\n",
      "|   data_id| 4.081508165850559|\n",
      "|     etnia| 4.076126036555841|\n",
      "|  dumb_cat|0.2845468150802483|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_iv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age_quant', 'educ_quant', 'data_id', 'etnia']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'educ', 'data_id_woe', 'etnia_woe']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical as woe True\n",
    "[s[:-6] if s.endswith('_quant') else s+'_woe' for s in cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'educ']\n",
      "['data_id', 'etnia']\n"
     ]
    }
   ],
   "source": [
    "# categorical as woe False\n",
    "num_selectec_features = list(filter(None, [s[:-6] if s.endswith('_quant') else None for s in cols_to_keep]))\n",
    "\n",
    "cat_selectec_features = list(filter(None, [None if s.endswith('_quant') else s for s in cols_to_keep]))\n",
    "\n",
    "print(num_selectec_features)\n",
    "print(cat_selectec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+--------+-----+\n",
      "|index|             data_id|etnia|dumb_cat|treat|\n",
      "+-----+--------------------+-----+--------+-----+\n",
      "|    0|                CPS1| marr|       b|    0|\n",
      "|    3|Dehejia-Wahba Sample|black|       b|    1|\n",
      "|    7|                CPS1|other|       b|    0|\n",
      "|   10|                CPS1| marr|       b|    0|\n",
      "|   12|                CPS1| marr|       a|    0|\n",
      "+-----+--------------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "| feature|       feature_value|                 woe|                  iv|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "| data_id|Dehejia-Wahba Sample| -4.0928282617041365|   4.024510946258392|\n",
      "| data_id|                CPS1|                 0.0|                 0.0|\n",
      "|   etnia|               other|  0.4487629426410833|0.038510203869811445|\n",
      "|   etnia|               black|  -2.254043853831219|  1.6506230367881973|\n",
      "|   etnia|                marr|   2.994244065051656|  1.7212600705769427|\n",
      "|   etnia|                hisp|                 0.0|                 0.0|\n",
      "|dumb_cat|                   a|0.029860773527106543|8.776845743563007E-4|\n",
      "|dumb_cat|                   b| -3.5050415968020174| 0.10302214506257643|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"A\", \"B\").pivot(\"C\").sum(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "| feature|                 iv|\n",
      "+--------+-------------------+\n",
      "| data_id|  4.024510946258392|\n",
      "|   etnia| 3.4103933112349516|\n",
      "|dumb_cat|0.10389982963693273|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_woe.groupBy('feature').agg(F.sum('iv').alias('iv')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+\n",
      "|etnia_treat|   0|  1|\n",
      "+-----------+----+---+\n",
      "|      other| 782|  5|\n",
      "|      black| 283| 27|\n",
      "|       marr|1994|  1|\n",
      "|       hisp| 236|  0|\n",
      "+-----------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_test.select(target).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Baseline Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Features Vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Class Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instanciating Classifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on Test Data and Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.95s/it]                                   \n"
     ]
    }
   ],
   "source": [
    "base_line_out = baseline_binary_classfiers(\n",
    "    dfs=dfs_train,\n",
    "    id_col='index',\n",
    "    target_col='treat',\n",
    "    num_features=['age', 'educ', 'nodegree', 're74', 're75', 're78', 'age2', 'age3', 'educ2', 'educ_re74', 'u74', 'u75'],\n",
    "    cat_features=['data_id', 'etnia'],\n",
    "    dfs_test=dfs_test,\n",
    "    weight_on_target=True,\n",
    "    log_mlflow_run=False,\n",
    "    artifact_stage_path = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GBTClassifier'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_line_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'metrics', 'decile_metrics'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_line_out['LogisticRegression'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confusion_matrix':    treat  prediction  count\n",
       " 0    0.0         0.0   3240\n",
       " 1    1.0         1.0     33\n",
       " 2    0.0         1.0     55,\n",
       " 'accuracy': 0.9834735576923077,\n",
       " 'f1': 0.5454545454545454,\n",
       " 'precision': 0.375,\n",
       " 'recall': 1.0,\n",
       " 'aucroc': 0.9916540212443096,\n",
       " 'aucpr': 0.375}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_line_out['LogisticRegression']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
