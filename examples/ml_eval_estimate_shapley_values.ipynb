{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Shap Values\n",
    "\n",
    "This notebook presentd the usage of the `estimate_individual_shapley_values()` function. It is based on the algorithm described in [interpretable-ml-book](https://christophm.github.io/interpretable-ml-book/shapley.html#estimating-the-shapley-value) and the implementation presented [here](https://medium.com/mlearning-ai/machine-learning-interpretability-shapley-values-with-pyspark-16ffd87227e3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_ds_toolbox.ml.data_prep import get_features_vector\n",
    "from pyspark_ds_toolbox.ml.eval import get_p1, estimate_individual_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/05 00:55:31 WARN Utils: Your hostname, matrix.local resolves to a loopback address: 127.0.0.1; using 10.0.0.105 instead (on interface en0)\n",
      "21/12/05 00:55:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/vini/Dev-Files/Poetry/virtualenvs/pyspark-ds-toolbox-H0pw_EKR-py3.8/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/12/05 00:55:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                .appName('Ml-Pipes') \\\n",
    "                .master('local[1]') \\\n",
    "                .config('spark.executor.memory', '3G') \\\n",
    "                .config('spark.driver.memory', '3G') \\\n",
    "                .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "                .config('spark.memory.offHeap.size', '3G') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file): \n",
    "    return pd.read_stata(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n",
    "\n",
    "df = read_data('nsw_mixtape.dta')\n",
    "df = pd.concat((df, read_data('cps_mixtape.dta')))\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = spark.createDataFrame(df.drop(columns=['data_id']))\\\n",
    "    .withColumn('age2', F.col('age')**2)\\\n",
    "    .withColumn('age3', F.col('age')**3)\\\n",
    "    .withColumn('educ2', F.col('educ')**2)\\\n",
    "    .withColumn('educ_re74', F.col('educ')*F.col('re74'))\\\n",
    "    .withColumn('u74', F.when(F.col('re74')==0, 1).otherwise(0))\\\n",
    "    .withColumn('u75', F.when(F.col('re75')==0, 1).otherwise(0))\n",
    "\n",
    "features=['age', 'age2', 'age3', 'educ', 'educ2', 'marr', 'nodegree', 'black', 'hisp', 're74', 're75', 'u74', 'u75', 'educ_re74']\n",
    "df_assembled = get_features_vector(df=df, num_features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.8\n",
    "train, test = df_assembled.randomSplit([train_size, (1-train_size)], seed=12345)\n",
    "\n",
    "row_of_interest = df_assembled.filter(F.col('index')==3).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in a Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "model_regressor = GBTRegressor(labelCol='re78')\n",
    "p_regression = Pipeline(stages=[model_regressor]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+\n",
      "|index|feature|              shap|\n",
      "+-----+-------+------------------+\n",
      "|    3|    age| 1531.361699634618|\n",
      "|    3|   age2|101.16142960996297|\n",
      "|    3|   age3| 102.3255325098817|\n",
      "|    3|   educ|-745.7361496945412|\n",
      "|    3|  educ2|109.55455718459126|\n",
      "+-----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_shap_regression = estimate_individual_shapley_values(\n",
    "    spark=spark,\n",
    "    df = df_assembled,\n",
    "    id_col='index',\n",
    "    model = p_regression,\n",
    "    problem_type='regression',\n",
    "    row_of_interest = row_of_interest,\n",
    "    feature_names = features,\n",
    "    features_col='features',\n",
    "    print_shap_values=False\n",
    ")\n",
    "\n",
    "sdf_shap_regression.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated re78 from shap values decomposition:\n",
      "10996.32116570435\n",
      "Observed re78:\n",
      "7506.14599609375\n"
     ]
    }
   ],
   "source": [
    "print('Estimated re78 from shap values decomposition:')\n",
    "print(df_assembled.select('re78').toPandas().re78.mean() + sdf_shap_regression.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "print('Observed re78:')\n",
    "v = df_assembled.filter(F.col('index')==3).select('re78').collect()[0][0]\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in a Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "model_classifier = GBTClassifier(labelCol='treat')\n",
    "p_classification = Pipeline(stages=[model_classifier]).fit(train)\n",
    "\n",
    "df_predicted = p_classification.transform(test)\\\n",
    "    .withColumn('p1', get_p1(F.col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------------+\n",
      "|index|feature|                shap|\n",
      "+-----+-------+--------------------+\n",
      "|    3|    age|-0.06047205754473...|\n",
      "|    3|   age2|-3.97297344350999...|\n",
      "|    3|   age3|4.368036381176134E-5|\n",
      "|    3|   educ|-0.00462527661687...|\n",
      "|    3|  educ2|-1.27617599632531...|\n",
      "+-----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_shap_classification = estimate_individual_shapley_values(\n",
    "    spark=spark,\n",
    "    df = df_predicted,\n",
    "    id_col='index',\n",
    "    model = p_classification,\n",
    "    problem_type='classification',\n",
    "    row_of_interest = row_of_interest,\n",
    "    feature_names = features,\n",
    "    features_col='features',\n",
    "    print_shap_values=False\n",
    ")\n",
    "\n",
    "sdf_shap_classification.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated treat prob from shap values decomposition:\n",
      "0.0523881502510601\n",
      "Estimated treat prob directly from model:\n",
      "0.043640851974487305\n"
     ]
    }
   ],
   "source": [
    "print('Estimated treat prob from shap values decomposition:')\n",
    "print(df_predicted.select('p1').toPandas().p1.mean() + sdf_shap_classification.select(F.sum('shap')).collect()[0][0])\n",
    "\n",
    "\n",
    "print('Estimated treat prob directly from model:')\n",
    "v = df_predicted.filter(F.col('index')==3).select('p1').collect()[0][0]\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "604c540002d7c1a602a115001e40004a700e7e1b29ae4331249882fda5e70a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pyspark-ds-toolbox-Fn-Rjt-3-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
